#!/usr/bin/env bash

# install the python package
pip install -e process/pkg

# parse txt files, unstem and convert them to json
mkdir data/clean

./process/clean/txt2json data/raw/mxm_dataset_train.txt \
    data/raw/mxm_reverse_mapping.txt data/clean/mxm_dataset_train.json

./process/clean/txt2json data/raw/mxm_dataset_test.txt \
    data/raw/mxm_reverse_mapping.txt data/clean/mxm_dataset_test.json

# join the two json files
./process/clean/join_json data/clean/mxm_dataset_train.json \
    data/clean/mxm_dataset_test.json data/clean/mxm_dataset.json

# convert json data to bag of words representation
mkdir data/transform

# Generate bag of words representation

# all words (~5000), stopwords removed (~2.3GB file)
# this file is used for exploring the distribution of the dataset
./process/transform/bag_of_words data/clean/mxm_dataset.json \
    data/transform/mxm_dataset.feather

# top 1k words, topwords removed, not normalized
./process/transform/bag_of_words data/clean/mxm_dataset.json \
    data/transform/mxm_dataset_1000.feather \
    --max_words 1000

# top 1k words, topwords removed and normalized (counts converted to
# proportions)
# this dataset is used for parts of the analysis that required computing
# distance measures between pairs
./process/transform/bag_of_words data/clean/mxm_dataset.json \
    data/transform/mxm_dataset_1000_normalized.feather \
    --max_words 1000 --normalize


# detect language - takes a while
./process/transform/language_detection \
    data/transform/mxm_dataset_1000.feather \
    data/transform/language.feather

# parse genre file
cat data/raw/msd_beatunes_map.cls | awk -F  '\t' '{ print $1 "\t" $2}' > data/clean/genre.tsv

# export track metadata: fields in the sqlite db and genre
./process/clean/export_track_metadata \
    data/raw/AdditionalFiles/track_metadata.db \
    data/clean/genre.tsv \
    data/raw/AdditionalFiles/artist_location.txt \
    data/transform/mxm_dataset_1000_normalized.feather \
    data/transform/track_metadata_tmp.feather

# fix artist id and name
./process/clean/fix_artist_name data/transform/track_metadata_tmp.feather \
    data/transform/track_metadata.feather

# TODO: impute language

# entity resolution for embeddings: take the glove dataset and get only
# the embeddings for the words in our dataset, fuzzy match the ones
# that do not match
./process/clean/subset_embeddings data/clean/mxm_dataset.json \
    data/raw/glove.6B/glove.6B.50d.txt data/clean/embeddings_subset.json

# build word embeddings representation, each song is represented as the
# sum of the count/proportion * embedding vectors for every word, run --help
# for more info
./process/transform/word_embeddings \
    data/transform/mxm_dataset_1000_normalized.feather \
    data/clean/embeddings_subset.json \
    data/transform/mxm_embeddings.feather


# join all datasets to create the ones that we will use

# bag of words with all the vocabulary (stopwords removed) + metadata
./process/transform/join data/transform/mxm_dataset.feather \
    data/transform/track_metadata.feather \
    data/transform/bag_of_words.feather


# top 1k bag of words (stopwords removed) + metadata
./process/transform/join data/transform/mxm_dataset_1000.feather \
    data/transform/track_metadata.feather \
    data/transform/bag_of_words_top_1000.feather

# top 1k bag of words (normalized and stopwords removed) + metadata
./process/transform/join data/transform/mxm_dataset_1000_normalized.feather \
    data/transform/track_metadata.feather \
    data/transform/bag_of_words_top_1000_normalized.feather

# word embeddings + metadata
./process/transform/join data/transform/mxm_embeddings.feather \
    data/transform/track_metadata.feather \
    data/transform/embeddings.feather


# TODO: compute json data for interactive component