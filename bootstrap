#!/usr/bin/env bash

# install the python package
pip install -e process/pkg

# parse txt files, unstem and convert them to json
mkdir data/clean
./process/clean/txt2json data/raw/mxm_dataset_train.txt \
    data/raw/mxm_reverse_mapping.txt data/clean/mxm_dataset_train.json
./process/clean/txt2json data/raw/mxm_dataset_test.txt \
    data/raw/mxm_reverse_mapping.txt data/clean/mxm_dataset_test.json

# join the two json files
./process/clean/join_json data/clean/mxm_dataset_train.json \
    data/clean/mxm_dataset_test.json data/clean/mxm_dataset.json

# convert json data to bag of words representation
mkdir data/transform

# Generate bag of words representation

# all words (~5000), stopwords removed (~10GB file)
# this file is used for exploring the distribution of the dataset
./process/transform/bag_of_words data/clean/mxm_dataset.json \
    data/transform/mxm_dataset.feather

# top 100 words, topwords removed andnormalized (counts converted to
# proportions)
# this dataset is used for parts of the analysis that required computing
# distance measures between pairs
./process/transform/bag_of_words data/clean/mxm_dataset.json \
    data/transform/mxm_dataset_50_normalized.feather \
    --max_words 100 --normalize

# parse genre file
cat data/raw/msd_beatunes_map.cls | awk -F  '\t' '{ print $1 "\t" $2}' > data/clean/genre.tsv

# export track metadata
./process/clean/export_track_metadata \
    data/raw/AdditionalFiles/track_metadata.db \
    data/clean/genre.tsv \
    data/raw/msd_beatunes_map.cls \
    data/transform/mxm_dataset_50_normalized.feather \
    data/transform/track_metadata.feather

# TODO: impute langauge and sentiment

# entity resolution for embeddings: take the glove dataset and get only
# the embeddings for the words in our dataset, fuzzy match the ones
# that do not match
./process/clean/subset_embeddings data/clean/mxm_dataset.json \
    data/raw/glove.6B/glove.6B.50d.txt data/clean/embeddings_subset.json

# build word embeddings representation, each song is represented as the
# sum of the count/proportion * embedding vectors for every word, run --help
# for more info
./process/transform/word_embeddings \
    data/transform/mxm_dataset_50_normalized.feather \
    data/clean/embeddings_subset.json \
    data/transform/mxm_embeddings_50_normalized.feather


# join all datasets to create a final one