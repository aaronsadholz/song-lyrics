{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import pprint\n",
    "\n",
    "#source: https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxm_dataset = pd.read_feather('mxm_dataset.feather')\n",
    "stop_words_tidytext = pd.read_feather('stop_words_tidytext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample the data for quick initial analysis\n",
    "tf_data = mxm_dataset.sample(frac= 1, random_state = 0).reset_index()\n",
    "features = tf_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove english stopwords from tidytext list\n",
    "stop_words = []\n",
    "for i in stop_words_tidytext.word:\n",
    "    if i in features:\n",
    "        stop_words.append(i)\n",
    "\n",
    "tf_data = tf_data.drop(stop_words, axis=1)\n",
    "tf_data = tf_data.drop(['track_id', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convery to tfidf to emphasize words that occur less frequently\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "features = tf_data.columns\n",
    "tf_data = csr_matrix(tf_data)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_data = tfidf.fit_transform(tf_data)\n",
    "tf_data = pd.DataFrame(tf_data.toarray(), columns=features)\n",
    "tfidf_data = pd.DataFrame(tfidf_data.toarray(), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=0, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#lda with tfidf\n",
    "lda_tfidf = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "lda_tfidf.fit(tfidf_data)\n",
    "\n",
    "#lda with  tf\n",
    "lda_tf = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "lda_tf.fit(tf_data)\n",
    "\n",
    "#nmf with if\n",
    "nmf_tfidf = NMF(n_components=10, random_state=0)\n",
    "nmf_tfidf.fit(tfidf_data)\n",
    "\n",
    "#nmf with if\n",
    "nmf_tf = NMF(n_components=10, random_state=0)\n",
    "nmf_tf.fit(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the top words for each topic\n",
    "def corpus_topics_top_words(model, features, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = [features[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows topic weights for each song\n",
    "def song_topics(model, song):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = sum(topic*song)\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'babi', 'time', 'feel', 'yeah', 'ca', 'gonna', 'heart', 'wanna', 'girl']\n",
      "1 ['de', 'el', 'la', 'en', 'te', 'mi', 'es', 'ich', 'tu', 'se']\n",
      "2 ['che', 'di', 'na', 'il', 'ja', 'la', 'se', 'mi', 'è', 'ma']\n",
      "3 ['i’m', 'don’t', 'it’', 'mari', 'refrain', 'you’r', 'warrior', '–', 'can’t', 'ye']\n",
      "4 ['love', 'time', 'feel', 'day', 'life', 'ca', 'eye', 'world', 'live', 'heart']\n",
      "5 ['nigga', 'ya', 'shit', 'fuck', 'rock', 'yo', 'em', 'yeah', 'bitch', 'wanna']\n",
      "6 ['jag', 'da', 'det', 'och', 'som', 'du', 'og', 'ba', 'på', 'är']\n",
      "7 ['la', 'je', 'de', 'les', 'le', 'pas', 'dan', 'des', 'qui', 'cest']\n",
      "8 ['god', 'death', 'lord', 'blood', 'soul', 'die', 'jesus', 'burn', 'dark', 'earth']\n",
      "9 ['christma', 'don', 'whoa', 'll', 'yea', 've', 'hallelujah', 'ni', 'wa', 'woah']\n"
     ]
    }
   ],
   "source": [
    "#tfidf, lda topic words\n",
    "top_per_topic_words = corpus_topics_top_words(lda_tfidf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'day', 'heart', 'night', 'feel', 'time', 'dream', 'eye', 'fall', 'alway']\n",
      "1 ['la', 'de', 'le', 'je', 'les', 'da', 'di', 'il', 'tu', 'che']\n",
      "2 ['love', 'babi', 'yeah', 'gonna', 'wanna', 'girl', 'hey', 'ooh', 'littl', 'gotta']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "4 ['nigga', 'ya', 'caus', 'rock', 'shit', 'boy', 'play', 'fuck', 'money', 'everybodi']\n",
      "5 ['na', 'de', 'eu', 'push', 'não', 'é', 'ik', 'um', 'doo', 'gimm']\n",
      "6 ['burn', 'run', 'dead', 'kill', 'fire', 'blood', 'die', 'black', 'head', 'death']\n",
      "7 ['ca', 'time', 'whi', 'tri', 'life', 'feel', 'caus', 'noth', 'wo', 'mind']\n",
      "8 ['de', 'el', 'la', 'en', 'te', 'mi', 'tu', 'se', 'es', 'yo']\n",
      "9 ['world', 'god', 'soul', 'lord', 'live', 'free', 'life', 'heaven', 'war', 'save']\n"
     ]
    }
   ],
   "source": [
    "#tf, lda topic words\n",
    "top_per_topic_words = corpus_topics_top_words(lda_tf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['ca', 'feel', 'whi', 'tri', 'believ', 'wo', 'caus', 'someth', 'noth', 'everyth']\n",
      "1 ['de', 'el', 'la', 'en', 'te', 'mi', 'tu', 'se', 'es', 'por']\n",
      "2 ['love', 'heart', 'true', 'girl', 'onli', 'kiss', 'forev', 'alway', 'hold', 'sweet']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'ein', 'mich']\n",
      "4 ['je', 'de', 'la', 'les', 'le', 'pas', 'des', 'dan', 'qui', 'à']\n",
      "5 ['babi', 'girl', 'ooh', 'night', 'pleas', 'littl', 'cri', 'tonight', 'babe', 'honey']\n",
      "6 ['yeah', 'gonna', 'wanna', 'girl', 'hey', 'nigga', 'ya', 'gotta', 'caus', 'fuck']\n",
      "7 ['che', 'di', 'la', 'il', 'è', 'mi', 'ma', 'da', 'ti', 'io']\n",
      "8 ['life', 'day', 'world', 'night', 'eye', 'dream', 'live', 'light', 'heart', 'fall']\n",
      "9 ['time', 'mind', 'wait', 'gonna', 'chang', 'wast', 'everi', 'day', 'mine', 'tri']\n"
     ]
    }
   ],
   "source": [
    "#tfidf, NMF topic words\n",
    "top_per_topic_words = corpus_topics_top_words(nmf_tfidf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['ca', 'feel', 'whi', 'tri', 'believ', 'wo', 'caus', 'someth', 'noth', 'everyth']\n",
      "1 ['de', 'el', 'la', 'en', 'te', 'mi', 'tu', 'se', 'es', 'por']\n",
      "2 ['love', 'heart', 'true', 'girl', 'onli', 'kiss', 'forev', 'alway', 'hold', 'sweet']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'ein', 'mich']\n",
      "4 ['je', 'de', 'la', 'les', 'le', 'pas', 'des', 'dan', 'qui', 'à']\n",
      "5 ['babi', 'girl', 'ooh', 'night', 'pleas', 'littl', 'cri', 'tonight', 'babe', 'honey']\n",
      "6 ['yeah', 'gonna', 'wanna', 'girl', 'hey', 'nigga', 'ya', 'gotta', 'caus', 'fuck']\n",
      "7 ['che', 'di', 'la', 'il', 'è', 'mi', 'ma', 'da', 'ti', 'io']\n",
      "8 ['life', 'day', 'world', 'night', 'eye', 'dream', 'live', 'light', 'heart', 'fall']\n",
      "9 ['time', 'mind', 'wait', 'gonna', 'chang', 'wast', 'everi', 'day', 'mine', 'tri']\n"
     ]
    }
   ],
   "source": [
    "#tf, NMF topic words\n",
    "top_per_topic_words = corpus_topics_top_words(nmf_tfidf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
