{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load bag bag_of_words_top_1000.feather')\n",
    "bag_of_words_top_1000 = pd.read_feather('../../data/transform/bag_of_words_top_1000.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_topics_top_words(model, features, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = [features[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict\n",
    "\n",
    "def song_topics(model, song):\n",
    "    topic_dict = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict.append(sum(topic*song))\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=25,\n",
       "             perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Identify topics within model')\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "mxm_25_top1000 = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "mxm_25_top1000.fit(bag_of_words_top_1000.iloc[:,16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237662/237662 [19:12<00:00, 206.19it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Identify per/song topic weights')\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_25_top1000,bag_of_words_top_1000.iloc[:,16:].columns.values, 10)\n",
    "\n",
    "#save per/song topic results to df\n",
    "song_topic_weights = np.zeros([len(bag_of_words_top_1000.iloc[:,16:]),25])\n",
    "temp = bag_of_words_top_1000.iloc[:,16:]\n",
    "for i in tqdm(range(len(bag_of_words_top_1000))):\n",
    "    song_weights = pd.Series(song_topics(mxm_25_top1000, temp.iloc[i]))\n",
    "    song_topic_weights[i] = song_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_topic_names = list(range(25))\n",
    "song_topic_weights_df = pd.DataFrame(data =song_topic_weights, columns=initial_topic_names)\n",
    "song_topic_weights_df['track_id'] = bag_of_words_top_1000['track_id_']\n",
    "df_topic_weights_reduced_df = song_topic_weights_df[['track_id',11,15,20]]\n",
    "df_topic_weights_reduced_df.columns = 'track_id', 'love','death','religion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': ['love', 'heart', 'sweet', 'true', 'give', 'enough', 'darling', 'touch', 'vision', 'found'], 'death': ['life', 'die', 'run', 'dead', 'kill', 'dream', 'blood', 'death', 'scream', 'clear'], 'religion': ['us', 'god', 'live', 'dance', 'people', 'heaven', 'hand', 'stand', 'angel', 'beautiful']}\n"
     ]
    }
   ],
   "source": [
    "#Top words in each topic\n",
    "topic_words = {}\n",
    "topic_words['love'] = top_per_topic_words[11]\n",
    "topic_words['death'] = top_per_topic_words[15]\n",
    "topic_words['religion'] = top_per_topic_words[20]\n",
    "print(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_top_1000['love'] = df_topic_weights_reduced_df['love']\n",
    "bag_of_words_top_1000['religion'] = df_topic_weights_reduced_df['religion']\n",
    "bag_of_words_top_1000['death'] = df_topic_weights_reduced_df['death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_top_1000 = pd.read_feather('../../data/transform/bag_of_words_top_1000.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_top_1000_topics = bag_of_words_top_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale by word count\n",
    "bag_of_words_top_1000_topics['sum'] = original_top_1000.iloc[:,16:].sum(axis=1)\n",
    "bag_of_words_top_1000_topics['love'] = bag_of_words_top_1000_topics['love']/bag_of_words_top_1000_topics['sum']\n",
    "bag_of_words_top_1000_topics['religion'] = bag_of_words_top_1000_topics['religion']/bag_of_words_top_1000_topics['sum']\n",
    "bag_of_words_top_1000_topics['death'] = bag_of_words_top_1000_topics['death']/bag_of_words_top_1000_topics['sum']\n",
    "bag_of_words_top_1000_topics.drop('sum', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank and scale between 0 and 1\n",
    "rank_order = np.array(range(len(bag_of_words_top_1000_topics)))/(len(bag_of_words_top_1000_topics)-1)\n",
    "\n",
    "bag_of_words_top_1000_topics = bag_of_words_top_1000_topics.sort_values(by='religion')\n",
    "bag_of_words_top_1000_topics = bag_of_words_top_1000_topics.reset_index(drop=True)\n",
    "bag_of_words_top_1000_topics['religion'] = rank_order\n",
    "\n",
    "bag_of_words_top_1000_topics = bag_of_words_top_1000_topics.sort_values(by='death')\n",
    "bag_of_words_top_1000_topics = bag_of_words_top_1000_topics.reset_index(drop=True)\n",
    "bag_of_words_top_1000_topics['death'] = rank_order\n",
    "\n",
    "bag_of_words_top_1000_topics = bag_of_words_top_1000_topics.sort_values(by='love')\n",
    "bag_of_words_top_1000_topics = bag_of_words_top_1000_topics.reset_index(drop=True)\n",
    "bag_of_words_top_1000_topics['love'] = rank_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag_of_words_top_1000_topics.to_feather('../../data/clean/bag_of_words_top_1000_weightedtopics.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yearly topics\n",
    "print('Identify topic weights each year')\n",
    "bag_with_year = bag_of_words_top_1000_topics[bag_of_words_top_1000_topics['release_year_'].notnull()].reset_index()\n",
    "\n",
    "weights_yearly_clean = {}\n",
    "weights_yearly_clean['song_count'] = [0] * len(bag_with_year['release_year_'].unique())\n",
    "weights_yearly_clean['love'] = [0] * len(bag_with_year['release_year_'].unique())\n",
    "weights_yearly_clean['death'] = [0] * len(bag_with_year['release_year_'].unique())\n",
    "weights_yearly_clean['religion'] = [0] * len(bag_with_year['release_year_'].unique())\n",
    "weights_yearly_clean['year'] = [0] * len(bag_with_year['release_year_'].unique())\n",
    "\n",
    "for i, val in enumerate(bag_with_year['release_year_'].unique()):\n",
    "    temp = bag_with_year[bag_with_year['release_year_']==val]\n",
    "    weights_yearly_clean['song_count'][i] = len(temp)\n",
    "    weights_yearly_clean['year'][i] = val\n",
    "    weights_yearly_clean['love'][i] = temp.love.mean()\n",
    "    weights_yearly_clean['death'][i] = temp.death.mean()\n",
    "    weights_yearly_clean['religion'][i] = temp.religion.mean()\n",
    "\n",
    "topics_per_year = pd.DataFrame(weights_yearly_clean).sort_values(by='year').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify topic weights for each artist ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20116it [18:19, 18.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#artist weights\n",
    "print('Identify topic weights for each artist (~21k iterations)')\n",
    "weights_artist_clean = {}\n",
    "weights_artist_clean['love'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['death'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['religion'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['artist_id'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['artist_name'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['longitude'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['latitude'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "weights_artist_clean['song_count'] = [0] * len(bag_of_words_top_1000_topics.artist_id_.unique())\n",
    "\n",
    "\n",
    "for i, val in tqdm(enumerate(bag_of_words_top_1000_topics.artist_id_.unique())):\n",
    "    temp = bag_of_words_top_1000_topics[bag_of_words_top_1000_topics.artist_id_==val]\n",
    "    weights_artist_clean['artist_id'][i] = val\n",
    "    weights_artist_clean['artist_name'][i] = temp.artist_name_.iloc[0]\n",
    "    weights_artist_clean['longitude'][i] = temp.longitude_.iloc[0]\n",
    "    weights_artist_clean['latitude'][i] = temp.latitude_.iloc[0]\n",
    "    weights_artist_clean['love'][i] = temp.love.mean()\n",
    "    weights_artist_clean['death'][i] = temp.death.mean()\n",
    "    weights_artist_clean['religion'][i] = temp.religion.mean()\n",
    "    weights_artist_clean['song_count'][i] = len(temp)\n",
    "\n",
    "topics_per_artist = pd.DataFrame(weights_artist_clean).sort_values(by='artist_name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_artist.to_feather('../../data/transform/artist_topic_weights.feather')\n",
    "topics_per_year.to_feather('../../data/transform/year_topic_weights.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
