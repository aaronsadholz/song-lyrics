{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import pprint\n",
    "\n",
    "#source: https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxm_dataset = pd.read_feather('mxm_dataset.feather')\n",
    "stop_words_tidytext = pd.read_feather('stop_words_tidytext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample the data for quick initial analysis\n",
    "tf_data = mxm_dataset.sample(frac= 1, random_state = 0).reset_index()\n",
    "features = tf_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove english stopwords from tidytext list\n",
    "stop_words = []\n",
    "for i in stop_words_tidytext.word:\n",
    "    if i in features:\n",
    "        stop_words.append(i)\n",
    "\n",
    "tf_data = tf_data.drop(stop_words, axis=1)\n",
    "tf_data = tf_data.drop(['track_id', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convery to tfidf to emphasize words that occur less frequently\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "features = tf_data.columns\n",
    "tf_data = csr_matrix(tf_data)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_data = tfidf.fit_transform(tf_data)\n",
    "tf_data = pd.DataFrame(tf_data.toarray(), columns=features)\n",
    "tfidf_data = pd.DataFrame(tfidf_data.toarray(), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=0, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#lda with tfidf\n",
    "lda_tfidf = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "lda_tfidf.fit(tfidf_data)\n",
    "\n",
    "#lda with  tf\n",
    "lda_tf = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "lda_tf.fit(tf_data)\n",
    "\n",
    "#nmf with if\n",
    "nmf_tfidf = NMF(n_components=10, random_state=0)\n",
    "nmf_tfidf.fit(tfidf_data)\n",
    "\n",
    "#nmf with if\n",
    "nmf_tf = NMF(n_components=10, random_state=0)\n",
    "nmf_tf.fit(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the top words for each topic\n",
    "def corpus_topics_top_words(model, features, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = [features[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows topic weights for each song\n",
    "def song_topics(model, song):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = sum(topic*song)\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'babi', 'time', 'feel', 'yeah', 'ca', 'gonna', 'heart', 'wanna', 'girl']\n",
      "1 ['de', 'el', 'la', 'en', 'te', 'mi', 'es', 'ich', 'tu', 'se']\n",
      "2 ['che', 'di', 'na', 'il', 'ja', 'la', 'se', 'mi', 'è', 'ma']\n",
      "3 ['i’m', 'don’t', 'it’', 'mari', 'refrain', 'you’r', 'warrior', '–', 'can’t', 'ye']\n",
      "4 ['love', 'time', 'feel', 'day', 'life', 'ca', 'eye', 'world', 'live', 'heart']\n",
      "5 ['nigga', 'ya', 'shit', 'fuck', 'rock', 'yo', 'em', 'yeah', 'bitch', 'wanna']\n",
      "6 ['jag', 'da', 'det', 'och', 'som', 'du', 'og', 'ba', 'på', 'är']\n",
      "7 ['la', 'je', 'de', 'les', 'le', 'pas', 'dan', 'des', 'qui', 'cest']\n",
      "8 ['god', 'death', 'lord', 'blood', 'soul', 'die', 'jesus', 'burn', 'dark', 'earth']\n",
      "9 ['christma', 'don', 'whoa', 'll', 'yea', 've', 'hallelujah', 'ni', 'wa', 'woah']\n"
     ]
    }
   ],
   "source": [
    "#tfidf, lda topic words\n",
    "top_per_topic_words = corpus_topics_top_words(lda_tfidf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'day', 'heart', 'night', 'feel', 'time', 'dream', 'eye', 'fall', 'alway']\n",
      "1 ['la', 'de', 'le', 'je', 'les', 'da', 'di', 'il', 'tu', 'che']\n",
      "2 ['love', 'babi', 'yeah', 'gonna', 'wanna', 'girl', 'hey', 'ooh', 'littl', 'gotta']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "4 ['nigga', 'ya', 'caus', 'rock', 'shit', 'boy', 'play', 'fuck', 'money', 'everybodi']\n",
      "5 ['na', 'de', 'eu', 'push', 'não', 'é', 'ik', 'um', 'doo', 'gimm']\n",
      "6 ['burn', 'run', 'dead', 'kill', 'fire', 'blood', 'die', 'black', 'head', 'death']\n",
      "7 ['ca', 'time', 'whi', 'tri', 'life', 'feel', 'caus', 'noth', 'wo', 'mind']\n",
      "8 ['de', 'el', 'la', 'en', 'te', 'mi', 'tu', 'se', 'es', 'yo']\n",
      "9 ['world', 'god', 'soul', 'lord', 'live', 'free', 'life', 'heaven', 'war', 'save']\n"
     ]
    }
   ],
   "source": [
    "#tf, lda topic words\n",
    "top_per_topic_words = corpus_topics_top_words(lda_tf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['ca', 'feel', 'whi', 'tri', 'believ', 'wo', 'caus', 'someth', 'noth', 'everyth']\n",
      "1 ['de', 'el', 'la', 'en', 'te', 'mi', 'tu', 'se', 'es', 'por']\n",
      "2 ['love', 'heart', 'true', 'girl', 'onli', 'kiss', 'forev', 'alway', 'hold', 'sweet']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'ein', 'mich']\n",
      "4 ['je', 'de', 'la', 'les', 'le', 'pas', 'des', 'dan', 'qui', 'à']\n",
      "5 ['babi', 'girl', 'ooh', 'night', 'pleas', 'littl', 'cri', 'tonight', 'babe', 'honey']\n",
      "6 ['yeah', 'gonna', 'wanna', 'girl', 'hey', 'nigga', 'ya', 'gotta', 'caus', 'fuck']\n",
      "7 ['che', 'di', 'la', 'il', 'è', 'mi', 'ma', 'da', 'ti', 'io']\n",
      "8 ['life', 'day', 'world', 'night', 'eye', 'dream', 'live', 'light', 'heart', 'fall']\n",
      "9 ['time', 'mind', 'wait', 'gonna', 'chang', 'wast', 'everi', 'day', 'mine', 'tri']\n"
     ]
    }
   ],
   "source": [
    "#tfidf, NMF topic words\n",
    "top_per_topic_words = corpus_topics_top_words(nmf_tfidf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['ca', 'feel', 'whi', 'tri', 'believ', 'wo', 'caus', 'someth', 'noth', 'everyth']\n",
      "1 ['de', 'el', 'la', 'en', 'te', 'mi', 'tu', 'se', 'es', 'por']\n",
      "2 ['love', 'heart', 'true', 'girl', 'onli', 'kiss', 'forev', 'alway', 'hold', 'sweet']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'ein', 'mich']\n",
      "4 ['je', 'de', 'la', 'les', 'le', 'pas', 'des', 'dan', 'qui', 'à']\n",
      "5 ['babi', 'girl', 'ooh', 'night', 'pleas', 'littl', 'cri', 'tonight', 'babe', 'honey']\n",
      "6 ['yeah', 'gonna', 'wanna', 'girl', 'hey', 'nigga', 'ya', 'gotta', 'caus', 'fuck']\n",
      "7 ['che', 'di', 'la', 'il', 'è', 'mi', 'ma', 'da', 'ti', 'io']\n",
      "8 ['life', 'day', 'world', 'night', 'eye', 'dream', 'live', 'light', 'heart', 'fall']\n",
      "9 ['time', 'mind', 'wait', 'gonna', 'chang', 'wast', 'everi', 'day', 'mine', 'tri']\n"
     ]
    }
   ],
   "source": [
    "#tf, NMF topic words\n",
    "top_per_topic_words = corpus_topics_top_words(nmf_tfidf, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'time', 'feel', 'ca', 'babi', 'yeah', 'day', 'life', 'caus', 'heart']\n",
      "1 ['la', 'de', 'en', 'el', 'tu', 'te', 'se', 'mi', 'es', 'ich']\n"
     ]
    }
   ],
   "source": [
    "lda_tf_2 = LatentDirichletAllocation(n_topics=2, random_state=0)\n",
    "lda_tf_2.fit(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'time', 'feel', 'ca', 'babi', 'yeah', 'day', 'life', 'caus', 'heart']\n",
      "1 ['la', 'de', 'en', 'el', 'tu', 'te', 'se', 'mi', 'es', 'ich']\n"
     ]
    }
   ],
   "source": [
    "top_per_topic_words = corpus_topics_top_words(lda_tf_2, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=5,\n",
       "             perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf_5 = LatentDirichletAllocation(n_topics=5, random_state=0)\n",
    "lda_tf_5.fit(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'world', 'life', 'eye', 'light', 'heart', 'dream', 'god', 'die', 'soul']\n",
      "1 ['la', 'de', 'en', 'el', 'tu', 'te', 'se', 'mi', 'le', 'si']\n",
      "2 ['babi', 'yeah', 'hey', 'ya', 'wanna', 'girl', 'nigga', 'rock', 'ooh', 'gonna']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'da', 'das', 'ist', 'es']\n",
      "4 ['love', 'time', 'ca', 'feel', 'day', 'caus', 'tri', 'whi', 'gonna', 'life']\n"
     ]
    }
   ],
   "source": [
    "top_per_topic_words = corpus_topics_top_words(lda_tf_5, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'heart', 'alway', 'hold', 'feel', 'onli', 'kiss', 'true', 'pleas', 'mine']\n",
      "1 ['na', 'da', 'di', 'che', 'la', 'il', 'se', 'mi', 'eu', 'ma']\n",
      "2 ['babi', 'ooh', 'woman', 'jag', 'det', 'babe', 'du', 'alright', 'crazi', 'och']\n",
      "3 ['world', 'god', 'heaven', 'war', 'angel', 'live', 'earth', 'king', 'fight', 'born']\n",
      "4 ['walk', 'blue', 'black', 'town', 'rememb', 'white', 'red', 'watch', 'morn', 'shine']\n",
      "5 ['run', 'call', 'friend', 'nobodi', 'beauti', 'lover', 'care', 'fool', 'river', 'push']\n",
      "6 ['die', 'burn', 'fire', 'ah', 'dead', 'hell', 'flame', 'citi', 'kill', 'devil']\n",
      "7 ['night', 'light', 'dream', 'sky', 'star', 'fli', 'wind', 'dark', 'alon', 'sleep']\n",
      "8 ['time', 'feel', 'life', 'ca', 'live', 'tri', 'mind', 'believ', 'day', 'chang']\n",
      "9 ['everyth', 'talk', 'anyth', 'easi', 'somebodi', 'honey', 'sorri', 'cos', 'drive', 'dem']\n",
      "10 ['yeah', 'everybodi', 'uh', 'mama', 'yea', 'bout', 'feelin', 'parti', 'brother', 'babi']\n",
      "11 ['ya', 'de', 'ik', 'en', 'van', 'je', 'bien', 'pa', 'dat', 'het']\n",
      "12 ['blind', 'death', 'lost', 'fear', 'control', 'learn', 'build', 'scream', 'becom', 'bird']\n",
      "13 ['fuck', 'nigga', 'shit', 'yo', 'em', 'ya', 'bitch', 'ai', 'yall', 'ass']\n",
      "14 ['day', 'home', 'sing', 'song', 'hear', 'play', 'peopl', 'time', 'lord', 'everi']\n",
      "15 ['gonna', 'whi', 'wo', 'wait', 'tonight', 'ca', 'someth', 'stop', 'cri', 'noth']\n",
      "16 ['girl', 'boy', 'happi', 'ladi', 'pretti', 'ja', 'ohh', 'ahead', 'treat', 'rainbow']\n",
      "17 ['wanna', 'danc', 'caus', 'ride', 'bad', 'beat', 'readi', 'shake', 'move', 'gonna']\n",
      "18 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "19 ['de', 'la', 'el', 'en', 'te', 'mi', 'tu', 'se', 'es', 'yo']\n",
      "20 ['kill', 'doe', 'ha', 'child', 'wild', 'gun', 'send', 'mani', 'dont', 'mother']\n",
      "21 ['la', 'de', 'le', 'je', 'les', 'pas', 'des', 'dan', 'qui', 'tu']\n",
      "22 ['hey', 'chorus', 'rock', 'roll', '2', 'jesus', 'repeat', '1', '&', 'doo']\n",
      "23 ['eye', 'fall', 'heart', 'soul', 'hand', 'sun', 'blood', 'breath', 'rain', 'tear']\n",
      "24 ['littl', 'gotta', 'sweet', 'sea', 'anoth', 'comin', 'bit', 'cmon', 'ba', 'fun']\n"
     ]
    }
   ],
   "source": [
    "lda_tf_25 = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "lda_tf_25.fit(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['love', 'heart', 'alway', 'hold', 'feel', 'onli', 'kiss', 'true', 'pleas', 'mine']\n",
      "1 ['na', 'da', 'di', 'che', 'la', 'il', 'se', 'mi', 'eu', 'ma']\n",
      "2 ['babi', 'ooh', 'woman', 'jag', 'det', 'babe', 'du', 'alright', 'crazi', 'och']\n",
      "3 ['world', 'god', 'heaven', 'war', 'angel', 'live', 'earth', 'king', 'fight', 'born']\n",
      "4 ['walk', 'blue', 'black', 'town', 'rememb', 'white', 'red', 'watch', 'morn', 'shine']\n",
      "5 ['run', 'call', 'friend', 'nobodi', 'beauti', 'lover', 'care', 'fool', 'river', 'push']\n",
      "6 ['die', 'burn', 'fire', 'ah', 'dead', 'hell', 'flame', 'citi', 'kill', 'devil']\n",
      "7 ['night', 'light', 'dream', 'sky', 'star', 'fli', 'wind', 'dark', 'alon', 'sleep']\n",
      "8 ['time', 'feel', 'life', 'ca', 'live', 'tri', 'mind', 'believ', 'day', 'chang']\n",
      "9 ['everyth', 'talk', 'anyth', 'easi', 'somebodi', 'honey', 'sorri', 'cos', 'drive', 'dem']\n",
      "10 ['yeah', 'everybodi', 'uh', 'mama', 'yea', 'bout', 'feelin', 'parti', 'brother', 'babi']\n",
      "11 ['ya', 'de', 'ik', 'en', 'van', 'je', 'bien', 'pa', 'dat', 'het']\n",
      "12 ['blind', 'death', 'lost', 'fear', 'control', 'learn', 'build', 'scream', 'becom', 'bird']\n",
      "13 ['fuck', 'nigga', 'shit', 'yo', 'em', 'ya', 'bitch', 'ai', 'yall', 'ass']\n",
      "14 ['day', 'home', 'sing', 'song', 'hear', 'play', 'peopl', 'time', 'lord', 'everi']\n",
      "15 ['gonna', 'whi', 'wo', 'wait', 'tonight', 'ca', 'someth', 'stop', 'cri', 'noth']\n",
      "16 ['girl', 'boy', 'happi', 'ladi', 'pretti', 'ja', 'ohh', 'ahead', 'treat', 'rainbow']\n",
      "17 ['wanna', 'danc', 'caus', 'ride', 'bad', 'beat', 'readi', 'shake', 'move', 'gonna']\n",
      "18 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "19 ['de', 'la', 'el', 'en', 'te', 'mi', 'tu', 'se', 'es', 'yo']\n",
      "20 ['kill', 'doe', 'ha', 'child', 'wild', 'gun', 'send', 'mani', 'dont', 'mother']\n",
      "21 ['la', 'de', 'le', 'je', 'les', 'pas', 'des', 'dan', 'qui', 'tu']\n",
      "22 ['hey', 'chorus', 'rock', 'roll', '2', 'jesus', 'repeat', '1', '&', 'doo']\n",
      "23 ['eye', 'fall', 'heart', 'soul', 'hand', 'sun', 'blood', 'breath', 'rain', 'tear']\n",
      "24 ['littl', 'gotta', 'sweet', 'sea', 'anoth', 'comin', 'bit', 'cmon', 'ba', 'fun']\n"
     ]
    }
   ],
   "source": [
    "top_per_topic_words = corpus_topics_top_words(lda_tf_25, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning topics\n",
    "\n",
    "After exploring a few basic models, here are my initial takeaways from the model I believe performed the best:\n",
    "* lda model\n",
    "* term frequency as the word representation\n",
    "* 25 topic counts.\n",
    "\n",
    "There is still a lot of room for improvement, but I think there is  quite a bit we could do with these topics as a supplemental part of this project. Each word is assigned a weight to each topic, so we can provide a score for each song (therefore also each artist, location, etc..) for how prevelent each topic is. Below are the top 10 weighted words in each topic, with the topic name I selected (I'm only selecting topics which appeared clear, there are another 13 topics at the bottom of this notebook). \n",
    "\n",
    "Would love to hear any feedback.\n",
    "\n",
    "Clear Topics:\n",
    "* **Love**: 'love', 'heart', 'alway', 'hold', 'feel', 'onli', 'kiss', 'true', 'pleas', 'mine'\n",
    "* **Pain/Loss/Fear**: 'blind', 'death', 'lost', 'fear', 'control', 'learn', 'build', 'scream', 'becom', 'bird'\n",
    "* **Religion**: 'world', 'god', 'heaven', 'war', 'angel', 'live', 'earth', 'king', 'fight', 'born'\n",
    "* **Death**: 'die', 'burn', 'fire', 'ah', 'dead', 'hell', 'flame', 'citi', 'kill', 'devil'\n",
    "* **Dancing**: 'wanna', 'danc', 'caus', 'ride', 'bad', 'beat', 'readi', 'shake', 'move', 'gonna'\n",
    "\n",
    "\n",
    "Clear Groupings that Don't Indicate a Topic:\n",
    "*  **\"sing/songy words**: 'na', 'da', 'di', 'che', 'la', 'il', 'se', 'mi', 'eu', 'ma'\n",
    "*  **Sleep/Night**: 'night', 'light', 'dream', 'sky', 'star', 'fli', 'wind', 'dark', 'alon', 'sleep'\n",
    "*  **Dutch?**: 'ya', 'de', 'ik', 'en', 'van', 'je', 'bien', 'pa', 'dat', 'het'\n",
    "*  **Hip/hop genre**:'fuck', 'nigga', 'shit', 'yo', 'em', 'ya', 'bitch', 'ai', 'yall', 'ass'\n",
    "*  **German**:'ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein'\n",
    "*  **Spanish**: 'de', 'la', 'el', 'en', 'te', 'mi', 'tu', 'se', 'es', 'yo'\n",
    "\n",
    "\n",
    "Topics I'd say are \"a bit of a stretch\":\n",
    "* **motivation/inpiration**: 'time', 'feel', 'life', 'ca', 'live', 'tri', 'mind', 'believ', 'day', 'chang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
