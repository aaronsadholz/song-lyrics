{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxm_dataset = pd.read_feather('../../data/transform/mxm_dataset.feather')\n",
    "mxm_dataset_1000 = pd.read_feather('../../data/transform/mxm_dataset_1000.feather')\n",
    "mxm_dataset_1000_normalized = pd.read_feather('../../data/transform/mxm_dataset_1000_normalized.feather')\n",
    "#mxm_embeddings = pd.read_feather('../../data/transform/mxm_embeddings.feather')\n",
    "#embeddings = pd.read_feather('../../data/transform/embeddings.feather')\n",
    "track_metadata = pd.read_feather('../../data/transform/track_metadata.feather')\n",
    "#bag_of_words = pd.read_feather('../../data/transform/bag_of_words.feather')\n",
    "#bag_of_words_top_1000 = pd.read_feather('../../data/transform/bag_of_words_top_1000.feather')\n",
    "#bag_of_words_top_1000_normalized = pd.read_feather('../../data/transform/bag_of_words_top_1000_normalized.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxm_10 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxm_25 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxm_10_top1000 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxm_25_top1000 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxm_10_top1000_norm complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/spare/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxm_25_top1000_norm complete\n"
     ]
    }
   ],
   "source": [
    "#extract topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#explore differnt data formats and topic counts\n",
    "mxm_10 = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "mxm_10.fit(mxm_dataset.drop('track_id', axis=1))\n",
    "print('mxm_10 complete')\n",
    "\n",
    "mxm_25 = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "mxm_25.fit(mxm_dataset.drop('track_id', axis=1))\n",
    "print('mxm_25 complete')\n",
    "\n",
    "mxm_10_top1000 = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "mxm_10_top1000.fit(mxm_dataset_1000.drop('track_id', axis=1))\n",
    "print('mxm_10_top1000 complete')\n",
    "\n",
    "mxm_25_top1000 = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "mxm_25_top1000.fit(mxm_dataset_1000.drop('track_id', axis=1))\n",
    "print('mxm_25_top1000 complete')\n",
    "\n",
    "mxm_10_top1000_norm = LatentDirichletAllocation(n_topics=10, random_state=0)\n",
    "mxm_10_top1000_norm.fit(mxm_dataset_1000_normalized.drop('track_id', axis=1))\n",
    "print('mxm_10_top1000_norm complete')\n",
    "\n",
    "mxm_25_top1000_norm = LatentDirichletAllocation(n_topics=25, random_state=0)\n",
    "mxm_25_top1000_norm.fit(mxm_dataset_1000_normalized.drop('track_id', axis=1))\n",
    "print('mxm_25_top1000_norm complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the top words for each topic\n",
    "def corpus_topics_top_words(model, features, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = [features[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows topic weights for each song\n",
    "def song_topics(model, song):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[topic_idx] = sum(topic*song)\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mxm_dataset.drop('track_id',axis=1).columns\n",
    "features_1000 = mxm_dataset_1000.drop('track_id',axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['eyes', 'light', 'die', 'life', 'burn', 'come', 'world', 'see', 'fall', 'soul']\n",
      "1 ['oh', 'yeah', 'love', 'na', 'ooh', 'lord', 'baby', 'ah', 'da', 'yes']\n",
      "2 ['day', 'one', 'god', 'sing', 'man', 'song', 'love', 'dream', 'heaven', 'old']\n",
      "3 ['e', 'che', 'di', 'non', 'il', 'la', 'un', 'mi', 'se', 'eu']\n",
      "4 ['de', 'la', 'je', 'le', 'et', 'les', 'en', 'pas', 'que', 'dans']\n",
      "5 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "6 ['like', 'get', 'got', 'man', 'ya', 'fuck', 'nigga', \"'cause\", 'know', 'shit']\n",
      "7 ['love', 'know', 'time', 'never', 'ca', 'want', 'would', 'feel', 'say', 'one']\n",
      "8 ['baby', 'go', 'come', 'got', 'gonna', 'get', 'girl', 'let', 'know', 'wanna']\n",
      "9 ['que', 'la', 'de', 'el', 'en', 'te', 'mi', 'tu', 'se', 'un']\n"
     ]
    }
   ],
   "source": [
    "#10 topics\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_10, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['day', 'away', 'sing', 'dream', 'rain', 'song', 'blue', 'sun', 'wind', 'waiting']\n",
      "1 ['na', 'lord', 'da', 'n', 'ba', 'doo', 'ni', 'ay', 'hallelujah', 'f']\n",
      "2 ['go', 'let', 'know', 'give', 'show', 'live', 'world', 'without', 'save', 'happy']\n",
      "3 ['et', 'les', 'le', 'que', 'pas', 'qui', 'pour', 'des', 'un', 'tu']\n",
      "4 ['la', 'de', 'je', 'à', 'tout', 'dans', 'du', 'mon', 'ne', 'moi']\n",
      "5 ['like', 'ya', 'nigga', 'fuck', 'shit', 'yo', \"'em\", 'got', \"'cause\", 'know']\n",
      "6 ['head', 'like', 'black', 'ride', 'take', 'watch', 'hand', 'gun', 'put', 'red']\n",
      "7 ['know', 'want', 'time', 'ca', 'say', 'way', 'make', 'take', 'never', 'think']\n",
      "8 ['come', 'night', 'light', 'tonight', 'turn', 'back', 'burn', 'around', 'like', 'fire']\n",
      "9 ['que', 'de', 'la', 'el', 'en', 'te', 'mi', 'tu', 'se', 'un']\n",
      "10 ['life', 'run', 'better', 'play', 'game', 'fool', 'done', 'fun', 'don?', 'dirty']\n",
      "11 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "12 ['would', 'one', 'could', 'said', 'never', 'like', 'day', 'back', 'home', 'well']\n",
      "13 ['good', 'call', 'yes', 'hear', 'name', 'heaven', '2', 'verse', 'real', '1']\n",
      "14 ['ah', 'white', 'im', 'hell', 'christmas', 'body', 'flesh', 'welcome', 'eat', 'machine']\n",
      "15 ['get', 'dance', 'boy', 'go', 'like', 'make', 'come', 'move', 'got', 'ready']\n",
      "16 ['en', 'jag', 'du', 'ja', 'det', 'ik', 'och', 'som', 'vi', 'og']\n",
      "17 ['got', 'gonna', 'hey', 'little', 'man', 'gotta', 'rock', 'roll', 'well', 'ai']\n",
      "18 ['e', 'di', 'che', 'non', 'un', 'il', 'mi', 'se', 'la', 'eu']\n",
      "19 ['god', 'fall', 'shine', 'stars', 'fly', 'sky', 'high', 'moon', 'sun', 'light']\n",
      "20 ['see', 'feel', 'eyes', 'lies', 'believe', 'inside', 'face', 'breath', 'beautiful', 'angel']\n",
      "21 ['heart', 'need', 'cry', 'gone', 'leave', 'alone', 'still', 'tears', 'long', 'hold']\n",
      "22 ['oh', 'baby', 'yeah', 'wanna', 'girl', 'ooh', 'alright', 'know', 'want', 'come']\n",
      "23 ['die', 'live', 'world', 'one', 'us', 'life', 'soul', 'kill', 'death', 'free']\n",
      "24 ['love', 'heart', 'kiss', 'sweet', 'true', 'one', 'mine', 'like', 'give', 'lover']\n"
     ]
    }
   ],
   "source": [
    "#25topics\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_25, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['go', 'come', 'back', 'let', 'one', 'hey', 'around', 'take', 'home', 'get']\n",
      "1 ['know', 'time', 'never', 'would', 'ca', 'say', 'way', 'could', 'see', 'think']\n",
      "2 ['que', 'la', 'de', 'el', 'en', 'mi', 'te', 'e', 'un', 'se']\n",
      "3 ['night', 'day', 'light', 'dream', 'like', 'away', 'sun', 'sky', 'sing', 'long']\n",
      "4 ['de', 'la', 'je', 'le', 'na', 'et', 'les', 'que', 'pas', 'qui']\n",
      "5 ['ich', 'und', 'die', 'du', 'da', 'der', 'nicht', 'das', 'im', 'ist']\n",
      "6 ['life', 'world', 'live', 'us', 'one', 'die', 'god', 'burn', 'soul', 'eyes']\n",
      "7 ['oh', 'yeah', 'gonna', 'ooh', 'ah', 'good', 'yes', 'king', 'ha', 'whoa']\n",
      "8 ['get', 'got', 'like', 'man', 'ya', 'know', \"'cause\", 'gotta', 'boy', 'fuck']\n",
      "9 ['love', 'baby', 'want', 'know', 'girl', 'feel', 'need', 'let', 'wanna', 'heart']\n"
     ]
    }
   ],
   "source": [
    "#10 topics top 1000 words\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_10_top1000, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['let', 'go', 'one', 'gotta', '2', 'somebody', 'verse', '1', 'take', 'time']\n",
      "1 ['see', 'eyes', 'time', 'heart', 'mind', 'inside', 'lies', 'away', 'feel', 'face']\n",
      "2 ['sing', 'call', 'chorus', 'song', 'hear', 'name', 'play', 'free', 'music', 'set']\n",
      "3 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ist', 'es', 'ein']\n",
      "4 ['like', 'look', 'walk', 'old', 'head', 'well', 'said', 'turn', 'new', 'town']\n",
      "5 ['get', 'gonna', 'right', 'better', 'ready', 'yea', 'got', 'time', 'magic', 'fun']\n",
      "6 ['world', 'fall', 'rain', 'without', 'ooh', 'easy', 'change', 'everyone', 'goes', 'wonder']\n",
      "7 ['want', 'ca', 'take', 'make', 'keep', 'stop', 'move', 'nobody', 'break', 'lose']\n",
      "8 ['got', 'girl', 'little', 'good', 'boy', 'rock', 'ah', 'alright', 'roll', 'shake']\n",
      "9 ['know', 'feel', 'like', 'need', 'way', 'give', 'things', \"'cause\", 'real', 'make']\n",
      "10 ['de', 'la', 'je', 'le', 'et', 'les', 'pas', 'dans', 'que', 'qui']\n",
      "11 ['love', 'heart', 'sweet', 'true', 'give', 'enough', 'darling', 'touch', 'vision', 'found']\n",
      "12 ['come', 'back', 'around', 'bring', 'round', 'sound', 'im', 'u', 'repeat', 'closer']\n",
      "13 ['hold', 'tonight', 'waiting', 'cry', 'kiss', 'forever', 'mine', 'together', 'arms', 'miss']\n",
      "14 ['oh', 'yeah', 'lord', 'yes', 'whoa', 'well', 'clear', 'say', 'belong', 'everywhere']\n",
      "15 ['life', 'die', 'run', 'dead', 'kill', 'dream', 'blood', 'death', 'scream', 'clear']\n",
      "16 ['would', 'never', 'say', 'could', 'know', 'think', 'time', 'things', 'try', 'one']\n",
      "17 ['que', 'la', 'de', 'el', 'te', 'en', 'tu', 'mi', 'se', 'un']\n",
      "18 ['na', 'da', 'e', 'eu', 'de', 'não', 'é', 'um', 'ba', 'ni']\n",
      "19 ['light', 'sky', 'sun', 'burn', 'fire', 'fly', 'shine', 'stars', 'high', 'soul']\n",
      "20 ['us', 'god', 'live', 'dance', 'people', 'heaven', 'hand', 'stand', 'angel', 'beautiful']\n",
      "21 ['baby', 'wanna', 'hey', 'tell', 'everybody', 'please', 'ooh', 'say', 'really', 'talk']\n",
      "22 ['day', 'time', 'night', 'away', 'long', 'home', 'gone', 'alone', 'every', 'stay']\n",
      "23 ['e', 'di', 'che', 'non', 'il', 'ha', 'un', 'mi', 'ja', 'jag']\n",
      "24 ['like', 'get', 'man', 'ya', 'got', 'fuck', 'nigga', 'shit', \"'cause\", \"'em\"]\n"
     ]
    }
   ],
   "source": [
    "#25 topics top 1000 words\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_25_top1000, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['come', 'like', 'love', 'day', 'know', 'time', 'night', 'go', 'see', 'away']\n",
      "1 ['la', 'pa', 'flow', 'ba', 'tan', 'las', 'va', 'ha', 'au', 'beautiful']\n",
      "2 ['one', 'life', 'time', 'see', 'come', 'world', 'like', 'know', 'eyes', 'live']\n",
      "3 ['e', 'che', 'di', 'na', 'non', 'da', 'la', 'un', 'il', 'mi']\n",
      "4 ['oh', 'baby', 'yeah', 'know', 'love', 'go', 'got', 'come', 'gonna', 'get']\n",
      "5 ['ja', 'ich', 'du', 'und', 'die', 'ei', 'en', 'der', 'nicht', 'das']\n",
      "6 ['get', 'like', 'got', 'know', 'go', 'back', 'one', 'man', 'come', 'time']\n",
      "7 ['de', 'que', 'la', 'en', 'el', 'te', 'tu', 'un', 'se', 'mi']\n",
      "8 ['love', 'know', 'heart', 'never', 'want', 'one', 'would', 'like', 'feel', 'need']\n",
      "9 ['know', 'time', 'never', 'go', 'see', 'say', 'ca', 'way', 'would', 'like']\n"
     ]
    }
   ],
   "source": [
    "#10 topics top 1000 words normalized\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_10_top1000_norm, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['away', 'day', 'time', 'know', 'run', 'go', 'come', 'rain', 'gone', 'like']\n",
      "1 ['que', 'de', 'la', 'el', 'en', 'te', 'mi', 'tu', 'un', 'se']\n",
      "2 ['burn', 'like', 'feel', 'fire', 'eyes', 'see', 'know', 'time', 'come', 'heart']\n",
      "3 ['cuz', \"lookin'\", 'evil', \"feelin'\", 'van', 'grave', 'tan', 'sans', 'grand', 'ass']\n",
      "4 ['de', 'je', 'la', 'et', 'le', 'les', 'dans', 'que', 'pas', 'un']\n",
      "5 ['would', 'could', 'never', 'know', 'like', 'love', 'time', 'one', 'say', 'see']\n",
      "6 ['life', 'one', 'god', 'time', 'come', 'see', 'world', 'live', 'like', 'death']\n",
      "7 ['know', 'want', 'go', 'ca', 'feel', 'time', 'let', 'way', 'see', 'say']\n",
      "8 ['gonna', 'got', 'get', 'go', 'know', 'rock', 'like', 'come', 'let', 'baby']\n",
      "9 ['dream', 'waiting', 'know', 'time', 'night', 'come', 'love', 'sleep', 'like', 'day']\n",
      "10 ['get', 'like', 'got', 'fuck', 'know', 'go', 'shit', 'nigga', 'one', 'back']\n",
      "11 ['dance', 'song', 'music', 'like', 'know', 'love', 'play', 'lyrics', 'get', 'go']\n",
      "12 ['got', 'man', 'like', 'know', 'one', 'boy', 'go', 'get', 'come', 'love']\n",
      "13 ['ja', 'ei', 'en', 'jag', 'du', 'se', 'det', 'och', 'som', 'og']\n",
      "14 ['lord', 'jesus', 'praise', 'king', 'sing', 'joy', 'glory', 'ring', 'story', 'hay']\n",
      "15 ['la', 'e', 'che', 'di', 'non', 'un', 'il', 'mi', 'per', 'si']\n",
      "16 ['like', 'love', 'come', 'see', 'know', 'light', 'sky', 'time', 'go', 'night']\n",
      "17 ['love', 'know', 'heart', 'one', 'like', 'oh', 'time', 'never', 'feel', 'say']\n",
      "18 ['na', 'da', 'ne', 'ni', 'se', 'mi', 'de', 'sa', 'ja', 'yo']\n",
      "19 ['baby', 'love', 'know', 'wanna', 'got', 'get', 'come', 'oh', 'time', 'go']\n",
      "20 ['ich', 'und', 'die', 'du', 'der', 'nicht', 'das', 'ein', 'ist', 'es']\n",
      "21 ['que', 'de', 'e', 'eu', 'não', 'é', 'um', 'se', 'pra', 'meu']\n",
      "22 ['know', 'love', 'like', 'please', 'need', 'time', 'say', 'someone', 'one', 'want']\n",
      "23 ['oh', 'yeah', 'hey', 'girl', 'know', 'love', 'got', 'like', 'little', 'baby']\n",
      "24 ['know', 'believe', 'time', 'see', 'never', 'one', 'change', 'lies', 'ca', 'life']\n"
     ]
    }
   ],
   "source": [
    "#25 topics top 1000 words normalized\n",
    "top_per_topic_words = corpus_topics_top_words(mxm_25_top1000_norm, features, 10)\n",
    "for i in list(top_per_topic_words.keys()):\n",
    "    print(str(i) + ' '+ str(top_per_topic_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning topics\n",
    "\n",
    "Best Performing model: LDA w/ unweighted, top 1000 words, 25 topics.\n",
    "\n",
    "Most clear 3 topics:\n",
    "\n",
    "11 - Love: ['love', 'heart', 'sweet', 'true', 'give', 'enough', 'darling', 'touch', 'vision', 'found']\n",
    "\n",
    "15 - Death: ['life', 'die', 'run', 'dead', 'kill', 'dream', 'blood', 'death', 'scream', 'clear']\n",
    "\n",
    "20 - Religion ['us', 'god', 'live', 'dance', 'people', 'heaven', 'hand', 'stand', 'angel', 'beautiful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_loc = pd.read_feather('../../data/transform/artist_location.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175234 entries, 0 to 175233\n",
      "Columns: 901 entries, index to cosa\n",
      "dtypes: float64(4), int16(888), int64(1), object(8)\n",
      "memory usage: 314.2+ MB\n"
     ]
    }
   ],
   "source": [
    ".info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
