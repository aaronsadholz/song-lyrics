# Main Analysis

## Data cleaning

On this section, we explain the process to go from the raw data to the clean
datasets we used for the analysis. All this process can be performed using the
`./bootstrap` script.

### Converting the data to a tabular format

The first step to work with the datasets was to put them in a better format
for cleaning.

The Musixmatch dataset is divided in two files .txt files (train and test),
these plain text files contain the counts for the 5,000 words. We first
converted both files to JSON using the `./txt2json` script and then combined
them in a single file, the output is has the following format:

```json
[
    {
        "track_id": "a track id"
        "bac_of_words": {
            "word_id_1": "count for word with id 1",
            "word_id_2": "count for word with id 2",
            ...
        }
    },
    ...
]
```

The raw data contains stemmed words, but the authors also provide a reverse
mapping to unstem them. We performed that operation in the same script.

After we got the data in JSON, we convert them to a binary format, we are
using [Apache Feather](https://blog.cloudera.com/blog/2016/03/feather-a-fast-on-disk-format-for-data-frames-for-r-and-python-powered-by-apache-arrow/) since
it has good interoperability with Python and R (this JSON to Feather format
is done using the  `./bag_of_words` script). The output looks like this:

+------------------+----------------------------------+----------------------------------+
| track_id         | word_id_1                        | word_id_2                        |
+==================+==================================+==================================+
| a track id       | count for word with id 1         | count for word with id 2         |
+------------------+----------------------------------+----------------------------------+
| another track id | another count for word with id 1 | another count for word with id 2 |
+------------------+----------------------------------+----------------------------------+
| ...              | ...                              | ...                              |
+------------------+----------------------------------+----------------------------------+


The `./bag_of_words` script contains some options. The dataset contains 5,000
words in total, we can limit the output to the top k words, normalize the
counts (convert them to proportions) and remove stopwords. We used these
options to generate several datasets for the analysis.

### Language detection

During the first iterations of the project we noticed that it is important
to know the language of the song. For example, when analyzing which artists
are far from each other in term of the words they use, we were just seeing
difference in language. For that reason we decided to detect the language
so we could use it for our analysis.

We do this using the [langdetect](https://github.com/Mimino666/langdetect) library (this is done in the `./language_detection`) script. This script
generates a `language.feather` file that maps songs with their language.

### Fixing artist name and ID

We performed some cleaning in the artist name and ID. We found that for
the same artist ID, some songs had more than one artist name, this happened
when some artist had collaborations. For example, an artist with ID `A1` may
have artist names `Noel Gallagher`, `Noel Gallagher; Richard Ashcroft`, `Noel Gallagher; Richard Ashcroft; Ian Brown`. We grouped the songs by artist ID
and assigned the most common artist name to all the songs.

After cleaning the name we notice another problem: some artist names had
more than one artist ID, this happened in a small number of cases but we cleaned the data as well. We grouped the songs by artist name and assigned
the first artist ID in the group. This problem may be due to artists changing
record labels, hence, not being recognized as the same artist by the Musixmatch portal.

### Extracting track metadata

There there are some other datasets that contain track metadata. It is
important to mention that the Musixmatch dataset (the one with the lyrics data) is a subset of the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) so we took the track IDs for such subset and only exported the metadata for those tracks.

The datasets that contain the track metadata are `track_metadata.db`
(included in the original raw data),  `msd_beatunes_map.cls` (we got that
data from [here](http://www.tagtraum.com/)) and `language.feather`. The
track metadata file is generated using the `./export_track_metadata` script and it contains the following columns (NAs information included):

* track_id - Unique identifier for the songs
* title - track title
* song_id - Another ID (undocumented)
* release â€“ Album name
* artist_id - Artist unique ID
* artist_mid - Musixmatch artist unique ID
* artist_name - Artist name
* duration - Track duration (seconds)
* artist_familiarity - Undocumented
* artist_hottnesss - Undocumented
* release_year - Track release year (26.27% NAs)
* genre - Artist genre (17.37% NAs)
* latitude - Artist latitude (58.29% NAs)
* longitude - Artist longitude (58.29% NAs)
* location - Location string such as "New York" (58.29% NAs)
* language - Song language (0.04% NAs)

### Generating clean datasets


## Word distribution

## What is the sentiment of the songs?

## Which are the topics? How do they change?

## Which bands are similar to each other?