# Main Analysis

## Data cleaning

On this section, we explain the process to go from the raw data to the clean
datasets we used for the analysis. All this process can be performed using the
`./bootstrap` script.

### Converting the data to a tabular format

The first step to work with the datasets was to put them in a better format
for cleaning.

The Musixmatch dataset is divided in two files .txt files (train and test),
these plain text files contain the counts for the 5,000 words. We first
converted both files to JSON using the `./txt2json` script and then combined
them in a single file, the output is has the following format:

```json
[
    {
        "track_id": "a track id"
        "bac_of_words": {
            "word_id_1": "count for word with id 1",
            "word_id_2": "count for word with id 2",
            ...
        }
    },
    ...
]
```

The raw data contains stemmed words, but the authors also provide a reverse
mapping to unstem them. We performed that operation in the same script.

After we got the data in JSON, we convert them to a binary format, we are
using [Apache Feather](https://blog.cloudera.com/blog/2016/03/feather-a-fast-on-disk-format-for-data-frames-for-r-and-python-powered-by-apache-arrow/) since
it has good interoperability with Python and R (this JSON to Feather format
is done using the  `./bag_of_words` script). The output looks like this:

+------------------+----------------------------------+----------------------------------+
| track_id         | word_id_1                        | word_id_2                        |
+==================+==================================+==================================+
| a track id       | count for word with id 1         | count for word with id 2         |
+------------------+----------------------------------+----------------------------------+
| another track id | another count for word with id 1 | another count for word with id 2 |
+------------------+----------------------------------+----------------------------------+
| ...              | ...                              | ...                              |
+------------------+----------------------------------+----------------------------------+


The `./bag_of_words` script contains some options. The dataset contains 5,000
words in total, we can limit the output to the top k words, normalize the
counts (convert them to proportions) and remove stopwords. We used these
options to generate several datasets for the analysis.

### Extracting track metadata

There there are some other datasets that contain track metadata. It is
important to mention that the Musixmatch dataset (the one with the lyrics data) is a subset of the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) so we took the track IDs for such subset and only exported the metadata for those tracks.

The datasets that contain the track metadata are `track_metadata.db`
(included in the original raw data) and `msd_beatunes_map.cls` (we got that
data from [here](http://www.tagtraum.com/)). The track metadata file is
generated using the `./export_track_metadata` script and it contains the
following columns:

* track_id - Unique identifier for the songs
* title - track title
* song_id - Another ID (undocumented)
* release â€“ Album name
* artist_id - Artist unique ID
* artist_mid - Musixmatch artist unique ID
* artist_name - Artist name
* duration - Track duration (seconds)
* Artist familiarity - Undocumented
* Artist hottnesss - Undocumented
* year - Track release year (26.27% NAs)
* latitude - Artist latitude
* longitude - Artist longitude
* location_string - Location string (such as "New York")

### Generating clean datasets

## Word distribution

## What is the sentiment of the songs?

## Which are the topics? How do they change?

## Which bands are similar to each other?